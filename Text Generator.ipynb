{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RCmRNYbH7Y1_"
      },
      "outputs": [],
      "source": [
        "# PROJECT TITLE - AI Text Generator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Dependencies\n",
        "!pip install transformers torch streamlit pyngrok==4.1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvyUeUzT7oEH",
        "outputId": "e0398f90-807f-4420-88f1-040045fdd368"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.51.0)\n",
            "Requirement already satisfied: pyngrok==4.1.1 in /usr/local/lib/python3.12/dist-packages (4.1.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from pyngrok==4.1.1) (1.0.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from pyngrok==4.1.1) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.29.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Packages & Download Models\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "4M3tpkzA7xGn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Sentiment Analysis Model\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        ")\n",
        "# Load Text Generation Model\n",
        "generator_name = \"gpt2\"\n",
        "generator_tokenizer = AutoTokenizer.from_pretrained(generator_name)\n",
        "generator_model = AutoModelForCausalLM.from_pretrained(generator_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8kMbl4z75kB",
        "outputId": "3364fadc-41c0-4912-b6b7-898fee2d0189"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Streamlit UI & Backend Logic\n",
        "app_code = '''\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# ================================\n",
        "# Load Models\n",
        "# ================================\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def load_models():\n",
        "    # Sentiment Classifier (DistilBERT)\n",
        "    sentiment_pipeline = pipeline(\n",
        "        \"sentiment-analysis\",\n",
        "        model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "    )\n",
        "\n",
        "    # GPT-2 Text Generator\n",
        "    generator_name = \"gpt2\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(generator_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(generator_name)\n",
        "\n",
        "    return sentiment_pipeline, tokenizer, model\n",
        "\n",
        "sentiment_pipeline, tokenizer, model = load_models()\n",
        "\n",
        "# ================================\n",
        "# Helper Functions\n",
        "# ================================\n",
        "def clean_prompt(text):\n",
        "    \"\"\"\n",
        "    Cleans the user prompt by removing instructions or meta-comments.\n",
        "    \"\"\"\n",
        "    lines = text.split(\"\\\\n\")\n",
        "    content_lines = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        # Remove lines with instruction keywords\n",
        "        if re.search(r\"\\\\b(write|be positive|if you|try|exercise|too long|not overly|uplifting|motivational)\\\\b\", line, re.IGNORECASE):\n",
        "            continue\n",
        "        content_lines.append(line)\n",
        "    return \" \".join(content_lines)\n",
        "\n",
        "def detect_sentiment(text):\n",
        "    \"\"\"\n",
        "    Detects sentiment using DistilBERT.\n",
        "    Returns 'positive', 'negative', or 'neutral'.\n",
        "    \"\"\"\n",
        "    result = sentiment_pipeline(text)[0]\n",
        "    label = result[\"label\"].lower()\n",
        "    if label == \"positive\":\n",
        "        return \"positive\"\n",
        "    elif label == \"negative\":\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "def generate_text(prompt, sentiment, max_length=200):\n",
        "    \"\"\"\n",
        "    Generates text aligned with the detected sentiment using GPT-2.\n",
        "    \"\"\"\n",
        "    instructions = {\n",
        "        \"positive\": \"Write a positive, uplifting paragraph about: \",\n",
        "        \"negative\": \"Write a negative, sad paragraph about: \",\n",
        "        \"neutral\": \"Write a neutral, factual paragraph about: \"\n",
        "    }\n",
        "    final_prompt = instructions.get(sentiment, instructions[\"neutral\"]) + prompt\n",
        "    input_ids = tokenizer.encode(final_prompt, return_tensors=\"pt\")\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        no_repeat_ngram_size=3,\n",
        "        do_sample=True\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# ================================\n",
        "# Streamlit UI\n",
        "# ================================\n",
        "st.set_page_config(\n",
        "    page_title=\"AI Sentiment-Based Text Generator\",\n",
        "    layout=\"wide\",\n",
        "    page_icon=\"ü§ñ\"\n",
        ")\n",
        "\n",
        "# Custom CSS\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        ".title { color: #4B0082; font-size: 36px; font-weight: bold; text-align: center; }\n",
        ".subtitle { color: #6A5ACD; font-size: 18px; text-align: center; }\n",
        ".sentiment { font-weight: bold; color: white; padding: 5px 10px; border-radius: 5px; display: inline-block; }\n",
        ".positive { background-color: #28a745; }\n",
        ".negative { background-color: #dc3545; }\n",
        ".neutral { background-color: #ffc107; color: black; }\n",
        ".generated-text { background-color: #f0f2f6; color: #000000; padding: 15px; border-radius: 10px; font-size: 16px; line-height: 1.6; white-space: pre-wrap; }\n",
        ".instructions { color: #444; font-size: 14px; margin-bottom: 10px; }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Page Header\n",
        "st.markdown('<div class=\"title\">üéØ AI Sentiment-Based Text Generator</div>', unsafe_allow_html=True)\n",
        "st.markdown('<div class=\"subtitle\">Enter a prompt and generate sentiment-aligned text instantly</div>', unsafe_allow_html=True)\n",
        "st.markdown(\"---\")\n",
        "\n",
        "# User Input\n",
        "st.subheader(\"üìù Enter Your Prompt\")\n",
        "st.markdown('<div class=\"instructions\">Instructions or extra guidance will be automatically removed for cleaner output.</div>', unsafe_allow_html=True)\n",
        "user_prompt = st.text_area(\"\", height=150, placeholder=\"Type your prompt here...\")\n",
        "\n",
        "st.subheader(\"üé≠ Sentiment Selection (Optional)\")\n",
        "sentiment_choice = st.selectbox(\"Choose sentiment (or Auto Detect):\", [\"Auto Detect\", \"positive\", \"negative\", \"neutral\"])\n",
        "\n",
        "st.subheader(\"üìè Generated Text Length\")\n",
        "max_len = st.slider(\"Select length (in tokens):\", min_value=50, max_value=500, value=200, step=10)\n",
        "\n",
        "# Generate Button\n",
        "if st.button(\"üöÄ Generate Text\"):\n",
        "    if not user_prompt.strip():\n",
        "        st.warning(\"‚ö†Ô∏è Please enter a prompt before generating text.\")\n",
        "    else:\n",
        "        with st.spinner(\"Cleaning prompt, detecting sentiment, and generating text...\"):\n",
        "            cleaned_prompt = clean_prompt(user_prompt)\n",
        "            sentiment = detect_sentiment(cleaned_prompt) if sentiment_choice==\"Auto Detect\" else sentiment_choice\n",
        "            generated = generate_text(cleaned_prompt, sentiment, max_length=max_len)\n",
        "\n",
        "        st.markdown(\"### üß† Detected Sentiment\")\n",
        "        st.markdown(f'<span class=\"sentiment {sentiment}\">{sentiment.upper()}</span>', unsafe_allow_html=True)\n",
        "        st.markdown(\"### üìù Generated Text\")\n",
        "        st.markdown(f'<div class=\"generated-text\">{generated}</div>', unsafe_allow_html=True)\n",
        "'''\n",
        "# Write to app.py\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "print(\"‚úÖ app.py created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4pBrK-S8SCr",
        "outputId": "abd1f9be-fa9d-4632-849b-c5c40de7e524"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ app.py created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Streamlit App Deployment\n",
        "\n",
        "# Install Streamlit + Ngrok v3\n",
        "!pip install -q streamlit groq python-docx pypdf\n",
        "\n",
        "# Download ngrok v3\n",
        "!wget -q -O ngrok.zip https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip\n",
        "!unzip -qo ngrok.zip\n",
        "!chmod +x ngrok\n",
        "!mv ngrok /usr/local/bin/ngrok\n",
        "\n",
        "# Configure Environment Variables\n",
        "import os, time, subprocess, requests\n",
        "\n",
        "# Groq API key\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "# Ngrok auth token\n",
        "NGROK_AUTH_TOKEN = \"2z0Oqv0tD166fELGCHwV2gLZwq1_2G2zUQRSs6C27k9vdzxwq\"\n",
        "!ngrok config add-authtoken $NGROK_AUTH_TOKEN\n",
        "\n",
        "# Create Log Directory\n",
        "LOG_DIR = \"/content/logs\"\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "\n",
        "# Kill previous sessions\n",
        "subprocess.run([\"pkill\", \"-f\", \"streamlit\"], stderr=subprocess.PIPE)\n",
        "subprocess.run([\"pkill\", \"-f\", \"ngrok\"], stderr=subprocess.PIPE)\n",
        "\n",
        "# Start Streamlit App\n",
        "APP_FILE = \"app.py\"\n",
        "!streamlit run $APP_FILE --server.port 8501 --server.address 0.0.0.0 > {LOG_DIR}/app_log.txt 2>&1 &\n",
        "print(\"üîÑ Starting Streamlit... please wait.\")\n",
        "time.sleep(7)\n",
        "\n",
        "# Start Ngrok Tunnel\n",
        "print(\"üîÑ Starting ngrok tunnel...\")\n",
        "ngrok_process = subprocess.Popen([\"ngrok\", \"http\", \"8501\"])\n",
        "time.sleep(5)\n",
        "\n",
        "# Fetch public URL\n",
        "try:\n",
        "    tunnel_info = requests.get(\"http://localhost:4040/api/tunnels\").json()\n",
        "    public_url = tunnel_info[\"tunnels\"][0][\"public_url\"]\n",
        "    print(\"üöÄ Your Groq-powered Streamlit App is LIVE at:\", public_url)\n",
        "except:\n",
        "    print(\"‚ùå Could not retrieve ngrok URL. Check logs below.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cISRWvd9mDe",
        "outputId": "5305709f-036f-45e8-fd02-663a5439b061"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n",
            "üîÑ Starting Streamlit... please wait.\n",
            "üîÑ Starting ngrok tunnel...\n",
            "üöÄ Your Groq-powered Streamlit App is LIVE at: https://68da548b298b.ngrok-free.app\n"
          ]
        }
      ]
    }
  ]
}